{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Importing necessary libraries and packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from datetime import datetime, timedelta, timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MongoToPostgresELT:\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Initialize Environment Variables and Database Connection\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the MongoToPostgresETL class.\n",
    "        Loads environment variables and sets up MongoDB and PostgreSQL connection parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        # Load environment variables from .env file\n",
    "        load_dotenv()\n",
    "\n",
    "        self.mongo_url = os.getenv(\"MONGO_DB_URL\")\n",
    "        self.mongo_db_database_name = os.getenv(\"MONGODB_DB_NAME\")\n",
    "        self.mongo_client = MongoClient(self.mongo_url)\n",
    "        self.db = self.mongo_client[\n",
    "            self.mongo_db_database_name\n",
    "        ]  # Initialize the database here\n",
    "\n",
    "        # PostgreSQL connection parameters from .env\n",
    "        self.pg_host = os.getenv(\"PG_HOST\")\n",
    "        self.pg_database = os.getenv(\"PG_DATABASE\")\n",
    "        self.pg_user = os.getenv(\"PG_USER\")\n",
    "        self.pg_password = os.getenv(\"PG_PASSWORD\")\n",
    "        self.pg_port = os.getenv(\"PG_PORT\")\n",
    "        self.pg_connection = None\n",
    "\n",
    "        self.unique_id_mapping = {\n",
    "            \"customers\": {\n",
    "                \"unique_id_key_col\": \"customer_id\",\n",
    "                \"table_name\": \"tbl_customers\",\n",
    "            },\n",
    "            \"loan_types\": {\n",
    "                \"unique_id_key_col\": \"loan_type_id\",\n",
    "                \"table_name\": \"tbl_loan_types\",\n",
    "            },\n",
    "            \"loan_applications\": {\n",
    "                \"unique_id_key_col\": \"loan_id\",\n",
    "                \"table_name\": \"tbl_loan_applications\",\n",
    "            },\n",
    "            \"loan_repayments\": {\n",
    "                \"unique_id_key_col\": \"repayment_id\",\n",
    "                \"table_name\": \"tbl_loan_repayments\",\n",
    "            },\n",
    "            \"loan_history\": {\n",
    "                \"unique_id_key_col\": \"history_id\",\n",
    "                \"table_name\": \"tbl_loan_history\",\n",
    "            },\n",
    "            \"loan_collateral\": {\n",
    "                \"unique_id_key_col\": \"collateral_id\",\n",
    "                \"table_name\": \"tbl_loan_collateral\",\n",
    "            },\n",
    "            \"loan_restructuring\": {\n",
    "                \"unique_id_key_col\": \"restructuring_id\",\n",
    "                \"table_name\": \"tbl_loan_restructuring\",\n",
    "            },\n",
    "            \"loan_disbursements\": {\n",
    "                \"unique_id_key_col\": \"disbursement_id\",\n",
    "                \"table_name\": \"tbl_loan_disbursements\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def get_mongo_client(self) -> MongoClient:\n",
    "        \"\"\"\n",
    "        Establishes and returns a MongoDB client connection.\n",
    "\n",
    "        Returns:\n",
    "        MongoClient: MongoDB client instance.\"\"\"\n",
    "\n",
    "        if self.mongo_client is None:\n",
    "            self.mongo_client = MongoClient(self.mongo_url)\n",
    "        return self.mongo_client\n",
    "\n",
    "    def connect_to_postgres(self):\n",
    "        \"\"\"\n",
    "        Establish a connection to the PostgreSQL database.\n",
    "\n",
    "        Returns:\n",
    "        connection: A connection object to the PostgreSQL database.\"\"\"\n",
    "\n",
    "        try:\n",
    "            connection = psycopg2.connect(\n",
    "                dbname=os.getenv(\"PG_DATABASE\"),\n",
    "                user=os.getenv(\"PG_USER\"),\n",
    "                password=os.getenv(\"PG_PASSWORD\"),\n",
    "                host=os.getenv(\"PG_HOST\"),\n",
    "                port=os.getenv(\"PG_PORT\"),\n",
    "            )\n",
    "            print(\"Connection to PostgreSQL established successfully.\")\n",
    "            return connection\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to connect to PostgreSQL: {e}\")\n",
    "            return None\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # load callection as dataframe to work on them as required.\n",
    "\n",
    "    def load_collection_as_dataframe(self, collection_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Converts a MongoDB collection into a pandas DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        collection_name (str): The name of the MongoDB collection.\n",
    "        db_name (str): The MongoDB database name.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: A DataFrame containing the MongoDB collection data.\"\"\"\n",
    "\n",
    "        client = self.get_mongo_client()\n",
    "        db = client[self.mongo_db_database_name]\n",
    "        collection = db[collection_name]\n",
    "        data = list(collection.find())\n",
    "\n",
    "        # Convert the data to a DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Check if '_id' column exists and drop it if it does\n",
    "        if \"_id\" in df.columns:\n",
    "            df = df.drop(columns=[\"_id\"])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def load_dataframe_to_postgres(\n",
    "        self, df: pd.DataFrame, table_name: str, json_columns: list = []\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Writes a pandas DataFrame to a PostgreSQL table, with support for JSON fields.\n",
    "\n",
    "        Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to be written to PostgreSQL.\n",
    "        table_name (str): The name of the PostgreSQL table where data will be inserted.\n",
    "        json_columns (list): List of column names in the DataFrame that contain JSON data.\n",
    "                            These columns will be converted to JSON strings and cast to 'jsonb' in PostgreSQL.\n",
    "        \"\"\"\n",
    "        # Establish connection for this operation\n",
    "        connection = self.connect_to_postgres()  # Always establish a new connection\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Handle conversion of specified columns to JSON strings\n",
    "        for column in json_columns:\n",
    "            df[column] = df[column].apply(json.dumps)\n",
    "\n",
    "        # Prepare the columns and placeholders for SQL query\n",
    "        columns = df.columns.tolist()\n",
    "\n",
    "        # Create SQL query for insertion, casting JSON columns to jsonb\n",
    "        insert_query = sql.SQL(\n",
    "            \"\"\"\n",
    "            INSERT INTO {} ({}) \n",
    "            VALUES ({})\"\"\"\n",
    "        ).format(\n",
    "            sql.Identifier(table_name),\n",
    "            sql.SQL(\", \").join(map(sql.Identifier, columns)),\n",
    "            sql.SQL(\", \").join(\n",
    "                sql.SQL(\"%s::jsonb\") if col in json_columns else sql.Placeholder()\n",
    "                for col in columns\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Convert DataFrame rows to tuples\n",
    "        data_tuples = [tuple(row) for row in df.itertuples(index=False)]\n",
    "\n",
    "        try:\n",
    "            # Execute the insertion query\n",
    "            cursor.executemany(insert_query, data_tuples)\n",
    "            connection.commit()\n",
    "            print(f\"Data successfully loaded into {table_name} table.\")\n",
    "        except Exception as error:\n",
    "            print(f\"Error inserting data into PostgreSQL: {error}\")\n",
    "            connection.rollback()\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # function to perfrom full load and incremental load\n",
    "\n",
    "    def perform_full_load(self, dataframes: dict):\n",
    "        \"\"\"\n",
    "        Loads all DataFrames from the provided dictionary into corresponding PostgreSQL tables.\n",
    "\n",
    "        Parameters:\n",
    "        dataframes (dict): A dictionary where keys are DataFrame names (e.g., 'customers__df')\n",
    "                           and values are the corresponding pandas DataFrames.\"\"\"\n",
    "\n",
    "        for df_name, df in dataframes.items():\n",
    "            table_name = df_name.replace(\n",
    "                \"__df\", \"\"\n",
    "            )  # Remove '__df' to get the table name\n",
    "\n",
    "            # Determine if any JSON columns need special handling\n",
    "            json_columns = (\n",
    "                []\n",
    "            )  # Define any specific columns that are JSON formatted if needed\n",
    "\n",
    "            # Check for JSON columns in the DataFrame\n",
    "            if \"new_loan_terms\" in df.columns or \"restructure_terms\" in df.columns:\n",
    "                json_columns = [\n",
    "                    \"new_loan_terms\",\n",
    "                    \"restructure_terms\",\n",
    "                ]  # Adjust based on your DataFrame structure\n",
    "\n",
    "            try:\n",
    "                self.load_dataframe_to_postgres(\n",
    "                    df, f\"tbl_{table_name}\", json_columns=json_columns\n",
    "                )  # Call the loading function\n",
    "                print(f\"Successfully loaded data into table: tbl_{table_name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading data into table {table_name}: {e}\")\n",
    "\n",
    "    def incremental_load(self):\n",
    "        \"\"\"\n",
    "        - Performs an incremental load for all MongoDB collections into corresponding PostgreSQL tables,\n",
    "        based on the defined unique_id_mapping.\n",
    "        - This function handles both new document insertions and updates for each collection, synchronizing changes with PostgreSQL tables.\n",
    "        \"\"\"\n",
    "        # Iterate over each collection in the unique_id_mapping\n",
    "        for collection_name, config in self.unique_id_mapping.items():\n",
    "            collection_unique_id_col = config[\"unique_id_key_col\"]\n",
    "            table_name = config[\"table_name\"]\n",
    "            table_unique_id_col = config[\"unique_id_key_col\"]\n",
    "\n",
    "            # Step 1: Fetch the latest 'added_at' and 'modified_at' from the PostgreSQL table\n",
    "            last_added_at, last_modified_at = self.get_latest_timestamps(table_name)\n",
    "\n",
    "            # Step 2: Load MongoDB collection as DataFrame for filtering\n",
    "            df = self.load_collection_as_dataframe(collection_name)\n",
    "\n",
    "            # Step 3: Filter for new and updated records in MongoDB\n",
    "            new_records = df[df[\"added_at\"] > last_added_at]\n",
    "            updated_records = df[\n",
    "                (df[\"modified_at\"] > last_modified_at)\n",
    "                & (df[\"added_at\"] <= last_added_at)\n",
    "            ]\n",
    "\n",
    "            # Step 4: Process new documents\n",
    "            if not new_records.empty:\n",
    "                for _, record in new_records.iterrows():\n",
    "                    # Check for document presence in MongoDB and insert if absent\n",
    "                    if not self.document_exists_in_mongo(\n",
    "                        collection_name,\n",
    "                        collection_unique_id_col,\n",
    "                        record[collection_unique_id_col],\n",
    "                    ):\n",
    "                        self.insert_document(collection_name, record.to_dict())\n",
    "                        # Insert new row into PostgreSQL\n",
    "                        self.load_dataframe_to_postgres(\n",
    "                            pd.DataFrame([record]), table_name\n",
    "                        )\n",
    "                    else:\n",
    "                        print(\n",
    "                            f\"Document with {collection_unique_id_col} {record[collection_unique_id_col]} already exists in MongoDB.\"\n",
    "                        )\n",
    "\n",
    "            # Step 5: Process updated documents\n",
    "            if not updated_records.empty:\n",
    "                for _, record in updated_records.iterrows():\n",
    "                    # Update document in MongoDB\n",
    "                    self.update_document_in_mongo(\n",
    "                        collection_name, record.to_dict(), collection_unique_id_col\n",
    "                    )\n",
    "                    # Update row in PostgreSQL\n",
    "                    self.update_record_in_postgres(\n",
    "                        record, table_name, table_unique_id_col\n",
    "                    )\n",
    "\n",
    "        print(\"Incremental load completed for all collections.\")\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # Function to Normalize tables and load into their repspective tables\n",
    "\n",
    "    def normalize_nested_fields(\n",
    "        self, df: pd.DataFrame, table_name: str, nested_fields: dict\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Normalizes nested fields in a DataFrame and inserts data into PostgreSQL.\n",
    "\n",
    "        Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing data with nested fields.\n",
    "        table_name (str): The name of the main PostgreSQL table.\n",
    "        nested_fields (dict): A dictionary where keys are nested fields in df,\n",
    "                            and values are the corresponding table names in PostgreSQL.\n",
    "        \"\"\"\n",
    "        # Iterate over each nested field\n",
    "        for nested_field, nested_table_name in nested_fields.items():\n",
    "            if nested_field in df.columns:\n",
    "                # Normalize the nested field into a separate DataFrame\n",
    "                nested_df = pd.json_normalize(df[nested_field])\n",
    "                nested_df.columns = [\n",
    "                    col.replace(\".\", \"_\") for col in nested_df.columns\n",
    "                ]  # Flatten column names\n",
    "\n",
    "                # Add unique IDs for the nested records\n",
    "                nested_df[f\"{nested_table_name}_id\"] = range(1, len(nested_df) + 1)\n",
    "\n",
    "                # Merge the IDs back to the main DataFrame\n",
    "                df = df.join(nested_df[f\"{nested_table_name}_id\"])\n",
    "\n",
    "                # Insert nested data into its respective table\n",
    "                self.load_dataframe_to_postgres(nested_df, nested_table_name)\n",
    "\n",
    "                # Drop original nested column from main DataFrame\n",
    "                df = df.drop(columns=[nested_field])\n",
    "\n",
    "        # Insert the main table with foreign keys for the nested tables\n",
    "        self.load_dataframe_to_postgres(df, table_name)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Helper Function to for insertion and updation into MongoDB\n",
    "\n",
    "    def insert_document(self, collection_name: str, document: dict):\n",
    "        \"\"\"\n",
    "        Inserts a document into the specified MongoDB collection only if\n",
    "        a document with the same unique identifier does not already exist.\n",
    "\n",
    "        Parameters:\n",
    "        collection_name (str): The name of the MongoDB collection.\n",
    "        document (dict): The document to insert into the collection.\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve unique identifier configuration for the collection\n",
    "        config = self.unique_id_mapping.get(collection_name)\n",
    "        if config is None:\n",
    "            print(\n",
    "                f\"No configuration found for collection '{collection_name}'. Document not inserted.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        # Extract the unique identifier column for the collection\n",
    "        unique_id_field = config[\"unique_id_key_col\"]\n",
    "\n",
    "        # Connect to collection\n",
    "        collection = self.db[collection_name]\n",
    "\n",
    "        # Check for existence of the document by unique identifier\n",
    "        if unique_id_field in document:\n",
    "            existing_doc = collection.find_one(\n",
    "                {unique_id_field: document[unique_id_field]}\n",
    "            )\n",
    "            if existing_doc:\n",
    "                print(\n",
    "                    f\"Document with {unique_id_field} = {document[unique_id_field]} already exists in collection '{collection_name}'.\"\n",
    "                )\n",
    "                return\n",
    "\n",
    "        # Set added_at and modified_at to the current date and time as per Indian Standard Time (IST)\n",
    "        current_time_ist = datetime.now(timezone.utc)\n",
    "        document[\"added_at\"] = current_time_ist\n",
    "        document[\"modified_at\"] = current_time_ist\n",
    "\n",
    "        # Insert the document if unique identifier check passes\n",
    "        result = collection.insert_one(document)\n",
    "        print(f\"Document inserted with ID: {result.inserted_id}\")\n",
    "\n",
    "    def update_document(self, collection_name: str, query: dict, update: dict):\n",
    "        \"\"\"\n",
    "        Updates an existing document in the specified MongoDB collection.\n",
    "\n",
    "        Parameters:\n",
    "        collection_name (str): The name of the MongoDB collection.\n",
    "        query (dict): The query to identify the document to update.\n",
    "        update (dict): The updates to apply to the document.\"\"\"\n",
    "\n",
    "        collection = self.db[collection_name]\n",
    "\n",
    "        # Set modified_at to the current date and time as per Indian Standard Time (IST)\n",
    "        current_time_ist = datetime.now(timezone.utc)\n",
    "        update[\"modified_at\"] = current_time_ist\n",
    "\n",
    "        result = collection.update_one(query, {\"$set\": update})\n",
    "        if result.matched_count > 0:\n",
    "            print(f\"Document updated: {result.modified_count} document(s) modified.\")\n",
    "        else:\n",
    "            print(\"No document found matching the query.\")\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Helper Function to for insertion and updation into PostgreSQL\n",
    "\n",
    "    def update_record_in_postgres(\n",
    "        self, record: pd.Series, table_name: str, unique_id_col: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Updates an existing record in the PostgreSQL table based on a unique identifier.\n",
    "\n",
    "        Parameters:\n",
    "        record (pd.Series): The record to update.\n",
    "        table_name (str): The name of the PostgreSQL table to update.\n",
    "        unique_id_col (str): The name of the unique identifier column used for the update.\n",
    "        \"\"\"\n",
    "\n",
    "        # Establish connection for this operation\n",
    "        connection = self.connect_to_postgres()\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Prepare the SQL UPDATE statement\n",
    "        set_clause = \", \".join(\n",
    "            [f\"{col} = %s\" for col in record.index if col != unique_id_col]\n",
    "        )  # Exclude unique identifier\n",
    "        update_query = f\"\"\"\n",
    "            UPDATE {table_name} \n",
    "            SET {set_clause} \n",
    "            WHERE {unique_id_col} = %s\n",
    "        \"\"\"\n",
    "\n",
    "        # Values to be updated\n",
    "        values = tuple(record[col] for col in record.index if col != unique_id_col) + (\n",
    "            record[unique_id_col],\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # Execute the update query\n",
    "            cursor.execute(update_query, values)\n",
    "            connection.commit()\n",
    "            print(\n",
    "                f\"Record with {unique_id_col} {record[unique_id_col]} updated successfully.\"\n",
    "            )\n",
    "        except Exception as error:\n",
    "            print(f\"Error updating record in PostgreSQL: {error}\")\n",
    "            connection.rollback()\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Helper Function to extract latest added_at and modified_at from PostgreSQL\n",
    "    def get_latest_timestamps(self, table_name):\n",
    "        \"\"\"\n",
    "        - Retrieves the latest added_at and modified_at timestamps from the specified PostgreSQL table.\n",
    "        - incremental_load() method is dependent on the data extracted by this method to perform as expected.\n",
    "        \"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT MAX(added_at) AS last_added_at, MAX(modified_at) AS last_modified_at\n",
    "        FROM {table_name};\n",
    "        \"\"\"\n",
    "        connection = self.connect_to_postgres()\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query)\n",
    "        result = cursor.fetchone()\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        return result[0], result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "elt = MongoToPostgresELT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of MongoDB collections\n",
    "collections = [\n",
    "    \"customers\",\n",
    "    \"loan_types\",\n",
    "    \"loan_applications\",\n",
    "    \"loan_repayments\",\n",
    "    \"loan_history\",\n",
    "    \"loan_collateral\",\n",
    "    \"loan_restructuring\",\n",
    "    \"loan_disbursements\",\n",
    "]\n",
    "\n",
    "# Dictionary to hold DataFrames\n",
    "collections_dict = {}\n",
    "\n",
    "# Loop through each collection name and load it into a DataFrame\n",
    "for collection in collections:\n",
    "    df_name = f\"{collection}\"  # Create a dynamic variable name\n",
    "    collections_dict[df_name] = elt.load_collection_as_dataframe(\n",
    "        collection\n",
    "    )  # Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------------------------------------------------------\n",
    "# collections_dict[\"customers\"][\"joined_date\"] = collections_dict[\"customers\"][\n",
    "#     \"joined_date\"\n",
    "# ].replace({pd.NaT: None})\n",
    "\n",
    "\n",
    "# # -----------------------------------------------------------------------------\n",
    "# collections_dict[\"loan_applications\"][\"application_date\"] = collections_dict[\n",
    "#     \"loan_applications\"\n",
    "# ][\"application_date\"].replace({pd.NaT: None})\n",
    "\n",
    "# collections_dict[\"loan_applications\"][\"approval_date\"] = collections_dict[\n",
    "#     \"loan_applications\"\n",
    "# ][\"approval_date\"].replace({pd.NaT: None})\n",
    "\n",
    "\n",
    "# # -----------------------------------------------------------------------------\n",
    "# collections_dict[\"loan_disbursements\"][\"disbursement_date\"] = collections_dict[\n",
    "#     \"loan_disbursements\"\n",
    "# ][\"disbursement_date\"].replace({pd.NaT: None})\n",
    "\n",
    "# collections_dict[\"loan_disbursements\"][\"application_date\"] = collections_dict[\n",
    "#     \"loan_disbursements\"\n",
    "# ][\"application_date\"].replace({pd.NaT: None})\n",
    "\n",
    "\n",
    "# # -----------------------------------------------------------------------------\n",
    "# collections_dict[\"loan_history\"][\"loan_disbursed_date\"] = collections_dict[\n",
    "#     \"loan_history\"\n",
    "# ][\"loan_disbursed_date\"].replace({pd.NaT: None})\n",
    "\n",
    "# collections_dict[\"loan_history\"][\"loan_repaid_date\"] = collections_dict[\"loan_history\"][\n",
    "#     \"loan_repaid_date\"\n",
    "# ].replace({pd.NaT: None})\n",
    "\n",
    "\n",
    "# # -----------------------------------------------------------------------------\n",
    "# collections_dict[\"loan_repayments\"][\"repayment_date\"] = collections_dict[\n",
    "#     \"loan_repayments\"\n",
    "# ][\"repayment_date\"].replace({pd.NaT: None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nat_with_none(collections_dict, columns_to_replace):\n",
    "    \"\"\"\n",
    "    Replaces pd.NaT with None in the specified columns of each collection in collections_dict.\n",
    "\n",
    "    Parameters:\n",
    "    collections_dict (dict): Dictionary where keys are collection names and values are DataFrames.\n",
    "    columns_to_replace (dict): Dictionary where keys are collection names and values are lists of columns\n",
    "                               in which to replace pd.NaT with None.\n",
    "    \"\"\"\n",
    "    for collection_name, columns in columns_to_replace.items():\n",
    "        for column in columns:\n",
    "            if column in collections_dict[collection_name].columns:\n",
    "                collections_dict[collection_name][column] = collections_dict[\n",
    "                    collection_name\n",
    "                ][column].replace({pd.NaT: None})\n",
    "\n",
    "\n",
    "# Define the columns to replace pd.NaT with None for each collection\n",
    "columns_to_replace = {\n",
    "    \"customers\": [\"joined_date\"],\n",
    "    \"loan_applications\": [\"application_date\", \"approval_date\"],\n",
    "    \"loan_disbursements\": [\"disbursement_date\", \"application_date\"],\n",
    "    \"loan_history\": [\"loan_disbursed_date\", \"loan_repaid_date\"],\n",
    "    \"loan_repayments\": [\"repayment_date\"],\n",
    "}\n",
    "\n",
    "# Replace pd.NaT with None in the specified columns of each collection\n",
    "replace_nat_with_none(collections_dict, columns_to_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to PostgreSQL established successfully.\n",
      "Data successfully loaded into tbl_customers table.\n",
      "Successfully loaded data into table: tbl_customers\n",
      "Connection to PostgreSQL established successfully.\n",
      "Data successfully loaded into tbl_loan_types table.\n",
      "Successfully loaded data into table: tbl_loan_types\n",
      "Connection to PostgreSQL established successfully.\n",
      "Data successfully loaded into tbl_loan_applications table.\n",
      "Successfully loaded data into table: tbl_loan_applications\n",
      "Connection to PostgreSQL established successfully.\n",
      "Data successfully loaded into tbl_loan_repayments table.\n",
      "Successfully loaded data into table: tbl_loan_repayments\n",
      "Connection to PostgreSQL established successfully.\n",
      "Data successfully loaded into tbl_loan_history table.\n",
      "Successfully loaded data into table: tbl_loan_history\n",
      "Connection to PostgreSQL established successfully.\n",
      "Data successfully loaded into tbl_loan_collateral table.\n",
      "Successfully loaded data into table: tbl_loan_collateral\n",
      "Connection to PostgreSQL established successfully.\n",
      "Data successfully loaded into tbl_loan_restructuring table.\n",
      "Successfully loaded data into table: tbl_loan_restructuring\n",
      "Connection to PostgreSQL established successfully.\n",
      "Data successfully loaded into tbl_loan_disbursements table.\n",
      "Successfully loaded data into table: tbl_loan_disbursements\n"
     ]
    }
   ],
   "source": [
    "# Perfrom Full Load\n",
    "\n",
    "elt.perform_full_load(collections_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 1 test into mongodb\n",
    "\n",
    "customer = {\n",
    "    \"customer_id\": 381617992991,\n",
    "    \"first_name\": \"Laurens\",\n",
    "    \"last_name\": \"Dana\",\n",
    "    \"gender\": \"Male\",\n",
    "    \"age\": 24,\n",
    "    \"employment_status\": \"employed\",\n",
    "    \"income_level\": \"low\",\n",
    "    \"location\": \"Phoenix\",\n",
    "    \"joined_date\": \"2020-04-07T05:30:00.000Z\",\n",
    "    \"added_at\": datetime.now(timezone.utc),\n",
    "    \"modified_at\": datetime.now(timezone.utc),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document inserted with ID: 672210af27e6422f0142dfb9\n"
     ]
    }
   ],
   "source": [
    "elt.insert_document(\"customers\", customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to PostgreSQL established successfully.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid comparison between dtype=datetime64[ns] and date",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidComparison\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:983\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 983\u001b[0m     other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_comparison_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidComparison:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:542\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin._validate_comparison_value\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(other):\n\u001b[1;32m--> 542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidComparison(other)\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(other) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;31mInvalidComparison\u001b[0m: 2024-10-25",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43melt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincremental_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 232\u001b[0m, in \u001b[0;36mMongoToPostgresELT.incremental_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    229\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_collection_as_dataframe(collection_name)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m# Step 3: Filter for new and updated records in MongoDB\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m new_records \u001b[38;5;241m=\u001b[39m df[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madded_at\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlast_added_at\u001b[49m]\n\u001b[0;32m    233\u001b[0m updated_records \u001b[38;5;241m=\u001b[39m df[\n\u001b[0;32m    234\u001b[0m     (df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodified_at\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m last_modified_at)\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madded_at\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m last_added_at)\n\u001b[0;32m    236\u001b[0m ]\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# Step 4: Process new documents\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:56\u001b[0m, in \u001b[0;36mOpsMixin.__gt__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__gt__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__gt__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6119\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6116\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   6117\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 6119\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:330\u001b[0m, in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    322\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLengths must match to compare\u001b[39m\u001b[38;5;124m\"\u001b[39m, lvalues\u001b[38;5;241m.\u001b[39mshape, rvalues\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    323\u001b[0m         )\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_extension_dispatch(lvalues, rvalues) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    326\u001b[0m     (\u001b[38;5;28misinstance\u001b[39m(rvalues, (Timedelta, BaseOffset, Timestamp)) \u001b[38;5;129;01mor\u001b[39;00m right \u001b[38;5;129;01mis\u001b[39;00m NaT)\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[0;32m    328\u001b[0m ):\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;66;03m# Call the method on lvalues\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_scalar(rvalues) \u001b[38;5;129;01mand\u001b[39;00m isna(rvalues):  \u001b[38;5;66;03m# TODO: but not pd.NA?\u001b[39;00m\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;66;03m# numpy does not like comparisons vs None\u001b[39;00m\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m operator\u001b[38;5;241m.\u001b[39mne:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:56\u001b[0m, in \u001b[0;36mOpsMixin.__gt__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__gt__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__gt__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:985\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m    983\u001b[0m     other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_comparison_value(other)\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidComparison:\n\u001b[1;32m--> 985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minvalid_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    987\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(other, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(dtype):\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;66;03m# We have to use comp_method_OBJECT_ARRAY instead of numpy\u001b[39;00m\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;66;03m#  comparison otherwise it would raise when comparing to None\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\invalid.py:40\u001b[0m, in \u001b[0;36minvalid_comparison\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     typ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(right)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid comparison between dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mleft\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtyp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid comparison between dtype=datetime64[ns] and date"
     ]
    }
   ],
   "source": [
    "elt.incremental_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
